---
title: "Google Play scrapper"
author: "Francisco W. Kerche"
date: '2022-07-08'
output:   
  html_document:
    theme: cosmo
    toc: yes
    toc_depth: 3
    toc_float: yes
---

# Trabalho de conclusão de curso de Data Scraping

Esse trabalho teve como intuito a construção de um scraper de algum site na internet. Como sou estudante de sociologia digital, pensei em fazer algo com as lojas de aplicativo, considerando a imensa quantidade de novos apps feitos diáriamente, me pareceu interessante fazer essa tentativa de extração na [loja de aplicativos do Google](https://play.google.com/store/games).

Dois projetos já consolidados foram minhas principais referências para fazer isso. Em primeiro lugar, o [YouTube Data Tools](https://tools.digitalmethods.net/netvizz/youtube/index.php), criado por Bernhard Rieder, que é um wrapper para lidar com a API do YouTube. Em segundo lugar, o trabalho de verão de Ana Marta Flores apresentado na #SMART Data Sprint, intitulado [What do gender representation and sexuality look like in Google Play Store?](https://smart.inovamedialab.org/editions/2022-discussing-methods-making/project-reports-2022/mapping-apps/). O primeiro me interessava pela possibilidade de cosntruir um scrapper que pudesse entrar em níveis (ou graus, como ele chama) de densidade no YouTube, observando não apenas quais canais seguem quais canais, ou mesmo quais vídeos seguem determinado vídeo, mas avançar nessa lógica em múltiplas camadas. Queria aproveitar das recomendações internas da plataforma do Google Play para isso, permitindo que observássemos a forma como a plataforma ordena seu conteúdo. Já o segundo grupo, utilizava desse método para ver quais os dados disponibilizados por cada aplicativo, o que também tentei raspar aqui.

Meu objetivo inicial era fazer a raspagem do site do ReclameAqui. Tinha interesse de fazer uma pesquisa sobre o reclame aqui de aplicativos de entrega, porém, o site estava bem difícil de conseguir acesso, e não me senti ainda apto pra avançar nele. Fui para o aplicativo do Google Play pensando originalmente em pegar os comentários dos aplicativos por lá, e achei interessante como proposta seguir por esse caminho.


## Informações gerais

Passei a olhar o site individualmente e tentar fazer um primeiro acesso com httr::GET(). Selecionei um aplicativo de jogos que minha irmã mais nova gosta e passei a ver se funcionava a entrada. O resultado de entrada foi 200, não foi necessário replicar a conexão e foi bastante simples de entrar. Os desafios seriam mais para frente para a busca de determinados dados, ou em especial para transformar isso num workflow simples para pesquisa.

```{r}

pacman::p_load(tidyverse, httr, lubridate, xml2, rvest, webdriver, tidygraph, ggraph, knitr)

# a few exemples to use
u <- "https://play.google.com/store/apps/details?id=com.roblox.client&gl=US"

app_h <- httr::GET(u)

# Get description
app_info <- app_h %>%
  read_html()
```

A primeira tarefa foi inspecionar a página e encontrar o XPATH referente aos valores que me interessavam. Tentei ser o mais abrangente possível, tentando pegar a ampla maioria das informações disponíveis na página. Como já tinha conseguido acesso anteriormente, atribuí o HTML para a variável app_info, para depois passar a buscar os percursos para os mais diversos valores.

```{r}

app_title <- app_info %>%
  xml2::xml_find_first("//h1[@class = 'Fd93Bb ynrBgc xwcR9d']") %>%
  xml2::xml_text()

app_score <- app_info %>%
  xml2::xml_find_first("//div[@class = 'TT9eCd']") %>%
  xml2::xml_text() %>%
  str_remove("star") %>%
  as.numeric()

app_description <- app_info %>%
  xml2::xml_find_first("//div[@class = 'bARER']") %>%
  xml2::xml_text()

last_update <- app_info %>%
  xml2::xml_find_first("//div[@class = 'xg1aie']") %>%
  xml2::xml_text()

tags <- app_info %>%
  xml2::xml_find_all("//span[@jsname = 'V67aGc']") %>%
  xml_text()

```

Em seguida, passei a buscar pela produtora, tanto o link para o site dela dentro do aplicativo, quanto para a produtora. Em análises futuras, penso em fazer um scraper que possa pegar todos os jogos de determinado desenvolvedor para análise.

A possibilidade de entrar no link da produtora permitiria construir análises por esse percurso também, mas achei mais interessante focar apenas nos aplicativos em si por enquanto.

```{r}

prod_link <- app_info %>%
  xml2::xml_find_first("//div[@class = 'Vbfug auoIOc']/a[contains(@href, 'store')]") %>%
  html_attr('href')

prod_name <- app_info %>%
  xml2::xml_find_first("//div[@class = 'Vbfug auoIOc']") %>%
  xml_text()

app_producer <- tibble(prod_link = prod_link,
                       producer = prod_name) %>%
  mutate(link = str_c('https://play.google.com', prod_link))



```


Foquei em pegar tanto o nome quanto o link para a produtora, que se encontravam antes no mesmo local, porém, o link não era encontrável apenas com o html_attr('href'), e para localizá-lo, utilizei do contains() para selecionar o href certo. Quando montei a tabela, adicionei a base do google para que fosse de fácil acesso se tivesse interesse de iterar sobre essa variável em algum momento. O mesmo processo seria feito com todos os hrefs encontrados na extração.


```{r}


app_overview <- tibble(name = app_title,
                   description = app_description,
                   updated = last_update,
                   tags = list(tags),
                   evaluation = app_score) %>%
  bind_cols(app_producer)

kable(app_overview, caption = "Visualização de um aplicativo")

```


Feitas essas duas etapas, criei um data-set final com todos os resultados em apenas uma linha. O intuito de fazer esse data-set "largo" (wide) e não "longo" (long) é a possibilidade de unificar uma série de aplicativos em apenas uma tabela que fique claro suas relações. Essa função vai ser particularmente útil na função final que vai ser produzida que busca fazer um "crawl" em busca das recomendações dos aplicativos. Para ver se esse procedimento era reproduzível em múltiplos aplicativos e não apenas no que criei originalmente, fiz um map que iterasse em um conjunto de jogos previamente selecionados.


```{r}


get_app_info <- function(url){
  
  app_h <- httr::GET(url)
  
  # Get description
  app_info <- app_h %>%
    read_html()
  
  
  app_title <- app_info %>%
    xml2::xml_find_first("//h1[starts-with(@class, 'Fd93Bb')]") %>%
    xml2::xml_text()
  
  app_score <- app_info %>%
    xml2::xml_find_first("//div[@class = 'TT9eCd']") %>%
    xml2::xml_text() %>%
    str_remove("star") %>%
    as.numeric()
  
  app_description <- app_info %>%
    xml2::xml_find_first("//div[@class = 'bARER']") %>%
    xml2::xml_text()
  
  last_update <- app_info %>%
    xml2::xml_find_first("//div[@class = 'xg1aie']") %>%
    xml2::xml_text()
  
  tags <- app_info %>%
    xml2::xml_find_all("//span[@jsname = 'V67aGc']") %>%
    xml_text()
  
  prod_link <- app_info %>%
    xml2::xml_find_first("//div[@class = 'Vbfug auoIOc']/a[contains(@href, 'store')]") %>%
    html_attr('href')
  
  prod_name <- app_info %>%
    xml2::xml_find_first("//div[@class = 'Vbfug auoIOc']") %>%
    xml_text()
  
  app_producer <- tibble(prod_link = prod_link,
                         producer = prod_name) %>%
    mutate(prod_link = str_c('https://play.google.com', prod_link))
  
  
  
  app_overview <- tibble(name = app_title,
                         description = app_description,
                         updated = last_update,
                         tags = list(tags),
                         evaluation = app_score,
                         links = url) %>%
    bind_cols(app_producer)
  
  return(app_overview)
}



```


```{r}

games <- c("https://play.google.com/store/apps/details?id=com.nianticlabs.pokemongo",
  "https://play.google.com/store/apps/details?id=com.ea.game.pvzfree_row",
  "https://play.google.com/store/apps/details?id=com.kitkagames.fallbuddies",
  "https://play.google.com/store/apps/details?id=com.kiloo.subwaysurf",
  "https://play.google.com/store/apps/details?id=fill.up.the.frigde.organization.games",
  "https://play.google.com/store/apps/details?id=com.dts.freefireth",
  "https://play.google.com/store/apps/details?id=com.wn.hairtattoo",
  "https://play.google.com/store/apps/details?id=com.noorgames.basketbattle",
  "https://play.google.com/store/apps/details?id=com.skyland.pet.realm.block.rain.craft",
  "https://play.google.com/store/apps/details?id=com.roblox.client",
  "https://play.google.com/store/apps/details?id=diy.makeup.master.artist.stylist")

informations <- map_df(games, get_app_info)
kable(informations, caption = "Detalhes de jogos no Google Play")


```

A extração funcionou tranquilamente, localizando todos os 11 jogos e suas informações especiais.

## Apps relacionados

A próxima etapa era pegar os aplicativos relacionados. No canto direito da página de cada um dos aplicativos encontramos um conjunto de aplicativos que são indicados pelo Google como "similares" do que o outro. Como podemos ver na seguinte imagem.


Aqui, encontramos duas principais informações, em related_apps, extraímos todas as informações visíveis, sendo elas o nome do aplicativo, o produtor e a avaliação. Porém, o link se encontra em outro path.

```{r}

related_apps <- app_info %>%
  xml2::xml_find_all("//div[@class = 'ubGTjb']") %>%
  xml_text() %>%
  .[!str_detect(., "([0-9\\.]+star|R\\$[0-9]+\\.[0-9]{2})")]

related_apps_link <- app_info %>%
  xml2::xml_find_all("//a[@class = 'Si6A0c nT2RTe']") %>%
  html_attr('href')

```

Porém, fazendo mais exemplos, passei a reparar que nem sempre encontravamos 3 valores perfeitamente, em especial, porque alguns aplicativs não tinham avaliações, e outros poderiam ter valores de custo, o que também poderia atraplhar a extração. Sem as avaliações ou com o preço a divisão não ficava clara, e considerando a dificuldade de dividir em um regex os produtores e os nomes dos aplicativos, achei mais simples eliminar ambas as informações. Para isso, utilizei de um string detect que eliminasse tanto estrelas quanto valores em reais. Isso fez com que tivessemos apenas duplas de valores, que serão depois fragmentados em name e producer. Para relacionar os valores de nome e produtores, utilizei da função get_pairs, que apenas buscaria os valores pares e ímpares e os colocariam nas posições corretas.

Em uma segunda busca, extraí o href relacionado ao aplicativo, permitindo unificar essas três informações em uma tabela simples de relações.



```{r}

  get_pairs <- function(.x){
    related_apps[seq(.x, length(related_apps), 2)]
  }
  relations <- tibble(name = get_pairs(1),
                      producer = get_pairs(2),
                      links = related_apps_link) %>%
    mutate(links = str_c("https://play.google.com", links))

kable(relations, caption = "Tabela de relação de aplicativos")


```

Para testar se esse procedimento funciona em outros aplicativos, transformei o processo em uma função, e fiz no mesmo grupo de 10 jogos, como foi feito anteriormente.


```{r}


get_app_relations <- function(url){
  
  app_h <- httr::GET({{url}})
  
  # Get description
  app_info <- app_h %>%
    read_html()

  app_info
  
  related_apps <- app_info %>%
    xml2::xml_find_all("//div[@class = 'ubGTjb']") %>%
    xml_text() %>%
    .[!str_detect(., "([0-9\\.]+star|R\\$[0-9]+\\.[0-9]{2})")]
  
  related_apps_link <- app_info %>%
    xml2::xml_find_all("//a[@class = 'Si6A0c nT2RTe']") %>%
    html_attr('href')
  
  get_pairs <- function(.x){
    related_apps %>%
      .[1:(length(related_apps_link)*2)] %>%
      .[seq(.x, length(.), by = 2)]
  }
  relations <- tibble(name = get_pairs(1),
                      producer = get_pairs(2),
                      links = related_apps_link) %>%
    mutate(links = str_c("https://play.google.com", links))
  
  return(relations)
}

try_get_app_relations <- possibly(get_app_relations, otherwise = NULL)
redes <- map_df(games, try_get_app_relations) %>%
  keep(~!is.null(.))
kable(redes, caption = "Conjunto de aplicativos e suas informações")

```

O problema que encontramos aqui é um NA em name para um jogo que não tem recomendações. Porém, isso não impede o script de rodar e pode ser rapidamente eliminado.

## Segurança digital

A próxima etapa é feita para pegar os valores dos dados disponibilizados pelos usuários para cada um dos perfis. Para encontrar esses dados é necessário clicar na seta ao lado de "Segurança de Dados" para encontrar uma página nova.

Procurei na página o que poderia me levar até esse outro site diretamente, e o que encontrei foi como o próprio link se altera. Vemos, por exemplo, como o link do aplicativo passa disso:

https://play.google.com/store/apps/details?id=com.dts.freefireth

Para isso

https://play.google.com/store/apps/datasafety?id=com.dts.freefireth

A principal diferença é que entre apps/ e ?, passamos de details para datasafety. O que foi feito aqui foi a mesma ideia, selecionar esse pedaço e alterar.

```{r}

link_id <- str_extract(u, "id=.*")

u_safe <- glue::glue("https://play.google.com/store/apps/datasafety?{link_id}")

l_safe <- GET(u_safe)

safe_r <- l_safe %>%
  read_html()

privacy_item <- safe_r %>%
  xml2::xml_find_all("//h3[@class = 'aFEzEb']") %>%
  html_text()

privacy_accept <- safe_r %>%
  xml2::xml_find_all("//div[@class = 'fozKzd']") %>%
  html_text()

app_privacy <- tibble(topic = privacy_item,
       requirements = privacy_accept) %>%
  separate_rows(requirements, sep = "( and |, and |, )")


```


Partindo dessa separação apenas repeti o processo de encontrar o xpath, e conseguia pegar tanto os ítens quanto os requerimentos. Os itens ficam em negrito e os requerimentos são aquilo que o app exige dentro daquele item, e por isso se cria a relação. Para separar em uma tabela apenas, foi feito um separate_rows, utilizando tanto vírgula, quanto "and", quanto "and, ", porque são as três formas possíveis que eles podem aparecer dependendo da quantidade de requerimentos.


Nesse caso também foi possível transformar em uma função e fazê-lo em múltiplos aplicativos.

```{r}


get_app_privacy <- function(url){
  link_id <- str_extract(url, "id=.*")
  
  u_safe <- glue::glue("https://play.google.com/store/apps/datasafety?{link_id}")
  
  l_safe <- GET(u_safe)
  
  safe_r <- l_safe %>%
    read_html()
  
  privacy_item <- safe_r %>%
    xml2::xml_find_all("//h3[@class = 'aFEzEb']") %>%
    html_text()
  
  privacy_accept <- safe_r %>%
    xml2::xml_find_all("//div[@class = 'fozKzd']") %>%
    html_text()
  
  app_privacy <- tibble(topic = privacy_item,
                        requirements = privacy_accept) %>%
    separate_rows(requirements, sep = "( and |, and |, )")
  return(app_privacy)
}

privacy <- map_df(games, get_app_privacy, .id = "origem") %>%
  left_join(tibble(app_name = games, origem = as.character(seq_along(games))))
  
kable(privacy, caption = "Topico e requerimentos do usuário por aplicativo")


```

## Avaliações

A última etapa do trabalho consistia em pegar as avaliações dos usuários. Como podemos ver as únicas avaliações disponíveis eram as três principais para cada aplicativo. Para mudar isso, era necessário clicar na seta ao lado de "Classificações e Resenhas". Porém, diferente da extração anterior, isso não abria uma página nova, mas uma série de partes ocultas do HTML.


Inspecionando o objeto das avaliações eu vi que elas tem o mesmo xpath das três avaliações finais. Porém, quando tentava extraír apenas pelo XPATH aparecia apenas as primeiras 3 avaliações. Tentei de diversas formas atingir essa parte oculta do HTML até perceber que realmente ela apenas seria encontrada se fosse clicado o botão. Para isso utilizeo do webdriver para entrar no site e clicar o botão.




```{r}


# webdriver::install_phantomjs()
pjs <- run_phantomjs()

# Fazendo isso, o phantomJS já está rodando

# A nossa porta de acesso já está rodando
ses <- Session$new(port = pjs$port) # o pacote session tem o método new, e a gente coloca a porta certa
# como se a gente tivesse clicado no botão para abrir o navegador

ses$go(u)
Sys.sleep(2)

botao <- ses$findElement(xpath = '//*[@id="yDmH0d"]/c-wiz[2]/div/div/div[1]/div[2]/div/div[1]/c-wiz[4]/section/header/div/div[2]/button')
botao$click()


html <- ses$getSource()

ses$delete()



```

O webdriver apenas entra no botão e joga a source para uma variável chamada html. Em seguida já fecha o phantom. Dessa forma, quando fosse fazer o XPATH novamente o resultado seria todos os comentários e não apenas os três primeiros.


```{r}

comment_text <- xml2::read_html(html) %>%
  xml2::xml_find_all("//div[@class='h3YV2d']") %>%
  xml2::xml_text()
comment_useful <- xml2::read_html(html) %>%
  xml2::xml_find_all("//div[@class='AJTPZc']") %>%
  xml2::xml_text()
stars <- xml2::read_html(html) %>%
  xml2::xml_find_all("//div[@role='img']") %>%
  html_attr("aria-label")

overall_star <- stars[2:6] %>%
  str_extract("[0-9.]+") %>%
  str_remove_all("\\.") %>%
  as.numeric()

general_eval <- tibble(stars = 5:1,
       evaluations = overall_star)
eval_fin <- general_eval %>%
  mutate(total_stars = stars*evaluations) %>%
  summarise(avarage = sum(total_stars)/sum(evaluations))

all_comments <- tibble(comment = comment_text,
                       stars = stars[7:length(stars)]) %>%
  mutate(stars = str_extract(stars, "[0-9]"))




```


O resultado aqui é múltiplo, ele apresenta qual a avaliação, o número de estrelas e também o total de avaliações para cada nota.

Tal qual todos os outros é possível fazer uma função para que isso seja feito programáticamente.

```{r}


get_app_evaluations <- function(url){
  
  # webdriver::install_phantomjs()
  pjs <- run_phantomjs()
  
  # Fazendo isso, o phantomJS já está rodando
  
  # A nossa porta de acesso já está rodando
  ses <- Session$new(port = pjs$port) # o pacote session tem o método new, e a gente coloca a porta certa
  # como se a gente tivesse clicado no botão para abrir o navegador
  
  ses$go(u)
  Sys.sleep(2)
  
  botao <- ses$findElement(xpath = '//*[@id="yDmH0d"]/c-wiz[2]/div/div/div[1]/div[2]/div/div[1]/c-wiz[4]/section/header/div/div[2]/button')
  botao$click()
  
  
  html <- ses$getSource()
  
  ses$delete()
  
  
  # readr::write_file(html, "output/all_comments.html")
  
  comment_text <- xml2::read_html(html) %>%
    xml2::xml_find_all("//div[@class='h3YV2d']") %>%
    xml2::xml_text()
  comment_useful <- xml2::read_html(html) %>%
    xml2::xml_find_all("//div[@class='AJTPZc']") %>%
    xml2::xml_text()
  stars <- xml2::read_html(html) %>%
    xml2::xml_find_all("//div[@role='img']") %>%
    html_attr("aria-label")
  
  overall_star <- stars[2:6] %>%
    str_extract("[0-9.]+") %>%
    str_remove_all("\\.") %>%
    as.numeric()
  
  general_eval <- tibble(stars = 5:1,
                         evaluations = overall_star)

  all_comments <- tibble(comment = comment_text,
                       stars = stars[7:length(stars)]) %>%
    mutate(stars = str_extract(stars, "[0-9]"))
  
  final <- list("avarage_star" = general_eval,
                "comments" = all_comments) 
  return(final)
}

evaluations <- map(games, get_app_evaluations) 
all_stars <- evaluations %>%
  map_df(~.$avarage_star, .id = "position") %>%
  left_join(tibble(position = as.character(seq_along(games)), game = games))

all_comments <- evaluations %>%
  map_df(~.$comments, .id = "position") %>%
  left_join(tibble(position = as.character(seq_along(games)), game = games))

final_eval <- list(avarage_star = all_stars,
     comments = all_comments)

```

## Graus de profundidade

Depois de ter feito as funções básicas e pegar as principais informações disponibilizadas por cada um dos aplicativos, é o momento de construir um scraper que consiga avançar em camadas de aplicativos. Utilizando as redes feitas pelas recomendações dos aplicativos, vamos fazer uma expansão em "grau", usando o método bola de neve. A principal ideia desse é tentar fazer de uma maneira que seja eficiente, entrando na internet o menor número de vezes para conseguir o mesmo resultado. Para isso, é necessário criar algumas novas funções.


Em primeiro lugar, vou transformar a função de download das relações de cada um dos canais, usando o get_app_info, que foi selecionado no primeiro grupo, é possível 

```{r}


connect_apps <- function(url, graph = F){
  original_info <- get_app_info({{url}})
  relate <- get_app_relations({{url}}) %>%
    select(links)
  relation_info <- map_df(relate$links, get_app_info)
  final_nodes <- relate %>%
    left_join(relation_info) %>%
    bind_rows(original_info)
  
  final_edges <- relate %>%
    select(to = links) %>%
    mutate(from = original_info$links)
  if(graph == F){
    output <-list("app_nodes" = final_nodes,
                  "app_edges" = final_edges) 
  return(output)
  } else{
  final_graph <- tidygraph::as_tbl_graph(final_edges) %N>%
    left_join(final_nodes)
  return(final_graph)
  }
}

connect_apps(games[1])

```

Em seguida, vamos criar um sistema para fazer isso em múltiplas camadas. Esse é uma das principais conquistas do trabalho. Ao final, temos um grafo com diversas informações sobre os aplicativos, e suas conexões estruturadas pela rede de recomendações do Google Play.

```{r}

crawl_app <- function(url, depth = 1, graph = F){
  
  try_connect_app <- possibly(connect_apps, otherwise = NULL)
  followers <- try_connect_app({{url}})
  if(is.null(followers)){
    usethis::ui_stop("This app can't bee the seed")}
  to_scrape <- followers$app_edges$to %>%
    unique()
  scraped <- followers$app_edges$from %>%
    unique()
  clean_scrape <- to_scrape[!to_scrape %in% scraped]
  
  usethis::ui_info("Start scrapping at depth 1")
  
  progressr::with_progress({
    p <- progressr::progressor(length(clean_scrape)) # O parâmetro é a quantidade de passos
      
    new_depth <- map(clean_scrape, 
                     ~{p()
                       try_connect_app(.)}) %>%
      keep(~!is.null(.))
    })
  
  split_edge_node <- function(.x, limit){
      final_edge <- .x %>%
        map(~.$app_edges) %>%
        map_df(filter, to %in% limit) %>%
        bind_rows(followers$app_edges) %>%
        unique()
      final_node <- .x %>%
        map(~.$app_nodes) %>%
        map_df(filter, links %in% limit) %>%
        bind_rows(followers$app_nodes) %>%
        unique()
      final <- list(app_nodes = final_node,
                    app_edges = final_edge)
      return(final)
    }
    
    if(depth == 1){
    
    all_apps <- c(to_scrape, scraped)
    
    final <- split_edge_node(new_depth, all_apps)
    
    
    } else if(depth == 2){
    
    
    targets <- new_depth %>%
      map(~.$app_edges$to) %>%
      unlist() %>%
      unique()
    
    origin_apps <- new_depth %>%
      map(~.$app_edges$from) %>%
      unlist() %>%
      unique() %>%
      c(., scraped)
    
    new_clean_scrape <- targets[!targets %in% origin_apps]

    usethis::ui_info("Start scrapping at depth 2")
    progressr::with_progress({
    p <- progressr::progressor(length(new_clean_scrape)) # O parâmetro é a quantidade de passos
      
    new_followers <- map(new_clean_scrape, ~{
      p()
      try_connect_app(.)
      }) %>%
      keep(~!is.null(.))
    })
    
    all_depth <- c(new_depth, new_followers)
    all_apps <- c(targets, origin_apps)
    
    final <- split_edge_node(all_depth, all_apps)
    }
    if(graph == T){
      final_graph <- tbl_graph(nodes = rename(final$app_nodes, name = links, app_name = name), edges = final$app_edges, directed = FALSE)
      
      return(final_graph)
    } else{
      return(final)
    }
}

teste_depth <- crawl_app(games[1], depth = 2, graph = T)




```

Isso permite construir grafos como esse das relações entre os aplicativos como esse, que se origina pelo app pokemon GO, e tem como valor de cor o produtor do aplicativo.

```{r}

teste_depth %N>%
  mutate(degree = centrality_degree(),
         big_label = ifelse(degree < 8, "", app_name)) %>%
  filter(degree > 1) %>%
  ggraph(layout = 'kk') +
  geom_edge_link() +
  geom_node_point(aes(size = degree, color = producer), show.legend = F) +
  geom_node_text(aes(label = big_label), size = 2)

```

Podemos fazer esse mesmo procedimento com múltiplas bases e unificar os grafos, permitindo que observemos o "ambiente" de recomendações para determinados aplicativos. O trabalho será base para pesquisas sobre o algoritmo de recomendação do Google Play e também segurança digital, observando as principais permissões de cada trabalho.


